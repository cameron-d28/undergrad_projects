{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`this notebook is an implementation of reinforcement learning without existing python frameworks. course ESE 6500: Learning in Robotics.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating environment class that the agent will interact with\n",
    "class Environment:\n",
    "    # 10 by 10 grid\n",
    "    def __init__(s):\n",
    "        s.grid_size = (10, 10)\n",
    "        s.num_states = s.grid_size[0] * s.grid_size[1]\n",
    "        s.R = 10\n",
    "        s.discount = 0.9\n",
    "\n",
    "    def init_map(s):\n",
    "        # create a matrix with border of 1s and center values of 0s\n",
    "        map = np.pad(np.zeros((s.grid_size[0]-2, s.grid_size[1]-2)), pad_width=1, mode='constant', constant_values=1)\n",
    "        # input values into the grid with respective rewards\n",
    "        map[[2, 4, 4, 4, 4, 4, 7, 8, 8, 3], [3, 4, 5, 6, 7, 7, 5, 8, 1, 3]] = [1, 1, 1, 1, 1, 1, 1, 2, 3, 4]\n",
    "\n",
    "        return map\n",
    "\n",
    "\n",
    "def policy_imp(env, transition_matrix, action_values, value_function):\n",
    "    argmax = lambda x: np.argmax(x)\n",
    "    policy = np.zeros(env.grid_size)\n",
    "\n",
    "    for row in range(env.grid_size[0]):\n",
    "        for col in range(env.grid_size[1]):\n",
    "            Q_vals = action_values[row, col]\n",
    "            T_vals = transition_matrix[row, col].reshape(4, -1)\n",
    "\n",
    "            policy[row, col] = argmax(\n",
    "                Q_vals\n",
    "                + (env.discount * T_vals @ value_function.reshape((-1, 1))).reshape(\n",
    "                    4\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def iter(env, T_matrix, action_values, initial_value_func, policy):\n",
    "    max_iterations = 300\n",
    "    value_func_history = np.zeros((max_iterations, env.grid_size[0], env.grid_size[1]))\n",
    "    value_func_history[0] = initial_value_func\n",
    "    for i in range(1, max_iterations):\n",
    "        for row in range(10):\n",
    "            for col in range(10):\n",
    "                value_func_history[i, row, col] = action_values[\n",
    "                    row, col, policy[row, col]\n",
    "                ] + env.discount * (\n",
    "                    T_matrix[row, col, policy[row, col]].flatten()\n",
    "                    @ value_func_history[i - 1].flatten().T\n",
    "                )\n",
    "\n",
    "    return value_func_history[-1]\n",
    "\n",
    "\n",
    "def init_transitions(env, obstacle_positions, goal_position):\n",
    "    # Actions: 0 - left, 1 - right, 2 - up, 3 - down\n",
    "    n = env.grid_size[0]\n",
    "    # represents all 4 different actions\n",
    "    transition_probs = np.zeros((n, n, 4, n, n))\n",
    "    goal_x, goal_y = goal_position\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            if (x, y) == goal_position:\n",
    "                transition_probs[goal_x, goal_y, :, goal_x, goal_y] = 1\n",
    "            elif (x, y) not in obstacle_positions:\n",
    "                # west (left)\n",
    "                transition_probs[x, y, 0, x, np.clip(y - 1, 0, n - 1)] += 0.7\n",
    "                transition_probs[x, y, 0, np.clip(x - 1, 0, n - 1), y] += 0.1\n",
    "                transition_probs[x, y, 0, np.clip(x + 1, 0, n - 1), y] += 0.1\n",
    "                transition_probs[x, y, 0, x, y] += 0.1\n",
    "\n",
    "                # east (right)\n",
    "                transition_probs[x, y, 1, x, np.clip(y + 1, 0, n - 1)] += 0.7\n",
    "                transition_probs[x, y, 1, np.clip(x - 1, 0, n - 1), y] += 0.1\n",
    "                transition_probs[x, y, 1, np.clip(x + 1, 0, n - 1), y] += 0.1\n",
    "                transition_probs[x, y, 1, x, y] += 0.1\n",
    "\n",
    "                # north (up)\n",
    "                transition_probs[x, y, 2, np.clip(x - 1, 0, n - 1), y] += 0.7\n",
    "                transition_probs[x, y, 2, x, np.clip(y + 1, 0, n - 1)] += 0.1\n",
    "                transition_probs[x, y, 2, x, np.clip(y - 1, 0, n - 1)] += 0.1\n",
    "                transition_probs[x, y, 2, x, y] += 0.1\n",
    "\n",
    "                # south (down)\n",
    "                transition_probs[x, y, 3, np.clip(x + 1, 0, n - 1), y] += 0.7\n",
    "                transition_probs[x, y, 3, x, np.clip(y + 1, 0, n - 1)] += 0.1\n",
    "                transition_probs[x, y, 3, x, np.clip(y - 1, 0, n - 1)] += 0.1\n",
    "                transition_probs[x, y, 3, x, y] += 0.1\n",
    "\n",
    "    return transition_probs\n",
    "\n",
    "\n",
    "def init_Q(env, grid):\n",
    "    obstacles = np.where(grid == 1)\n",
    "    goal = np.where(grid == 2)\n",
    "\n",
    "    Q = -1 * np.ones((env.grid_size[1], env.grid_size[0], 4))\n",
    "    Q[obstacles[0], obstacles[1], :] = -env.R\n",
    "    Q[goal[0], goal[1], :] = env.R\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def plot(V, policy, i, obstacle_positions, goal_position):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.set_xticks(np.arange(0.5, 10.5, 1))\n",
    "    ax.set_yticks(np.arange(0.5, 10.5, 1))\n",
    "    plt.imshow(V, cmap=\"viridis\")\n",
    "    ax.grid()\n",
    "\n",
    "    for x in range(10):\n",
    "        for y in range(10):\n",
    "            if (x, y) not in obstacle_positions and (x, y) not in goal_position:\n",
    "                if policy[x, y] == 0:\n",
    "                    ax.arrow(y, x, -0.25, 0, head_width=0.1)\n",
    "                elif policy[x, y] == 1:\n",
    "                    ax.arrow(y, x, 0.25, 0, head_width=0.1)\n",
    "                elif policy[x, y] == 2:\n",
    "                    ax.arrow(y, x, 0, -0.25, head_width=0.1)\n",
    "                elif policy[x, y] == 3:\n",
    "                    ax.arrow(y, x, 0, 0.25, head_width=0.1)\n",
    "    plt.title(f\"Value Function and Policy at Iteration {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = Environment()\n",
    "grid = env.init_map()\n",
    "\n",
    "0, 1, 2, 3  # left, right, up, down\n",
    "obstacle_locs = np.where(grid == 1)\n",
    "obstacle_positions = list(zip(*np.where(grid == 1)))\n",
    "goal_position = list(zip(*np.where(grid == 2)))\n",
    "\n",
    "transition_matrix = init_transitions(env, obstacle_positions, goal_position[0])\n",
    "Q_values = init_Q(env, grid)\n",
    "\n",
    "num_iterations = 4\n",
    "V = np.zeros(env.grid_size)\n",
    "policies = np.ones((num_iterations + 1, env.grid_size[0], env.grid_size[1]), dtype=int)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    updated_value_func = iter(env, \n",
    "        transition_matrix, Q_values, V, policies[i]\n",
    "    )\n",
    "    V = updated_value_func\n",
    "    policies[i + 1] = policy_imp(env, \n",
    "        transition_matrix, Q_values, updated_value_func\n",
    "    )\n",
    "    plot(updated_value_func, policies[i + 1], i + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
